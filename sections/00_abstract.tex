Our visual perception is characterized by spatio-temporal compositionality, allowing us to comprehend visual events as a composition of actions involving actors interacting with objects. While training vision models to exhibit such compositionality has been a long standing goal in Computer Vision, existing video benchmarks only test simple temporal logic. 
We build a pipeline to programmatically generate a benchmark by combining Action Genome's spatio-temporal scene graph annotations with handcrafted templates and programs.
With the pipeline, we introduce AGQA: a benchmark for compositional spatio-temporal reasoning. Utilizing language as the medium to test for spatio-temporal compositionality, AGQA presents $1.77M$ balanced and $133M$ unbalanced question answer pairs for $9.6K$ videos, which is orders of magnitude larger than existing benchmarks.
AGQA tests a wide variety of spatio-temporal reasoning skills, including action sequencing, temporal localization, human-object interaction recognition, length, repetition, and compositional reasoning.
Using AGQA, we evaluate modern visual reasoning systems, demonstrating that the best video question answering models barely perform better than non-visual baselines exploiting linguistic bias. Although some models generalize to longer video inputs, none of them generalize to novel compositions unseen during training.