%
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{lipsum}
\usepackage[table,xcdraw]{xcolor}

% Include other packages here, before hyperref.
\newcommand{\rak}[1]{{\color{red}{rak: #1}}}
\newcommand{\ma}[1]{{\color{purple}{ma: #1}}}
\newcommand{\mgm}[1]{{\color{cyan}{mgm: #1}}}

\definecolor{objectColor}{rgb}{0.8, 0.254, 0.145}
\newcommand{\object}[1]{{\color{objectColor}{#1}}}
\definecolor{relationshipColor}{rgb}{0.415, 0.658, 0.309}
\newcommand{\relationship}[1]{{\color{relationshipColor}{#1}}}
\definecolor{actionColor}{rgb}{0.964, 0.698, 0.419}
\newcommand{\action}[1]{{\color{actionColor}{#1}}}
\definecolor{timeColor}{rgb}{0.403, 0.305, 0.654}
\newcommand{\temporal}[1]{{\color{timeColor}{#1}}}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{2764} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Action Genome Question Answers: \\ A Benchmark for Compositional Spatio-Temporal Reasoning in Videos}

\author{Madeleine Grunde-McLaughlin\\
Stanford University\\
%Philadelphia, Pennsylvania\\
{\tt\small madeleine.grunde@gmail.com}

\and
Ranjay Krishna\\
Stanford University\\
%Palo Alto, California\\
{\tt\small ranjay.krishna@gmail.com}

\and
Maneesh Agrawala\\
Stanford University\\
%Palo Alto, California\\
{\tt\small maneesh@cs.stanford.edu}


}
\maketitle
\pagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
\input{sections/00_abstract}
\end{abstract}

%%%%%%%%% BODY TEXT

\input{sections/01_introduction}
\input{sections/02_related}
\input{sections/03_method}


\section{Experiments and analysis}

%CLEVR: First described models. They then analyzed the dataset by question type (query vs compare), relationship type (spatial vs attribute), question topology (chain vs tree-structured questions). they have an interesting section on looking at "effective" question size. They say that accuracy is not necessarily related to question size itself, so they prune functions from the program until they find the smallest functional program that when put on the scene graph creates the correct answer. This is the 'effective' program length, and that is related to difficulty. Nothing comes immediately to mind about how I would do that with AGQA though. They also looked at relative vs absolute spatial reasoning. SO if you say "left of x" they see how accurate it is if you just look in left half of the image, and then they prune when spatial relationships aren't absolutely necessary. They found that much worse on ones that require relative, not absolute, spatial reasoning. This could potentially be expanded to temporal localization. Then look at novel combinations. They have a summary in discussions and future work. 

%GQA: breaks down semantic and structural categories (does not spend much time on these),then talks about data set's vocabulary size and possible answers. We are a lot weaker on this point. GQA talks about briefly baseline experiments on blind models. They they describe transfer performance, i.e. training on VQA and testing on GQA and vice versa to show theirs is harder (we don't have an equivalent though). Then, they get into describing their new metrics and have. subsection for each category. For some of them they have a formal representation, for others they don't.

% Note to self: will need to change some to logical structural.. maybe compare?

%Notes to self: I think that the stuff CLEVR did with pruning down programs to find the essential necessities is cool. I probably don't have time for that now, but worth keeping in mind. 

First, we judge the quality of our questions through a human subject study, finding that humans achieve a 97\% accuracy on our benchmark. We then tested our suite of metrics on several recent models to measure different types of spatio-temporal reasoning skills. We found that models perform close to random on most measures. Finally, we measure compositional reasoning in several ways and find that existing models struggle on generalizing to new information, but are relatively more successful at generalizing from shorter videos to longer videos.

\subsection{Human Study}

As mentioned before, annotation errors in the Charades and Action Genome datasets could lead to confusing or incorrect questions in our dataset. To measure the level of error after our augmentation strategies, we tested Amazon Mechanical Turk workers on their ability to answer the questions. We ran the study on 300 questions over 60 videos, with two Amazon Mechanical Turk workers per video. When the two workers disagreed, we got a third worker's opinion to break the tie. Since the Charades videos were filmed in AMT worker's homes there were often many objects and actions that were not within the Charades and Action Genome annotations. To clarify the objects of interest, we listed them for the workers. Workers were first given a free form text box, then given a drop down menu of possible answers to select the closest possible answer from the words within the model's vocabulary. We measure human accuracy from accuracy with the drop down menu. 

After the first round, 28 questions had disagreement between the annotators so received a third opinion. With the tie breakers, human accuracy is 97\%, which implies that 3\% of our questions have errors in them. In comparison, GQA gets 89.3\% on human accuracy~\cite{hudson2019gqa}, and CLEVR gets between 79\% and 97\& depending on the question category~\cite{johnson2017clevr}. 


%\subsection{Describe the models we looked at briefly}

%To test different subsets of the dataset, we look at three state of the art models. The Hierarchical Conditional Relation Network (HCRN) consists of... 


%\mgm{Is it necessary to do this? CLEVR did, GQA did not}
%\begin{figure*}[t]
%    \begin{table}[]
%\input{novel_comp_results}
%        \caption{Caption}
%        \label{tab:my_label}
%    \end{table}
%\end{figure*}

\begin{figure}[t]
\begin{center}
\resizebox{\linewidth}{!}{
\input{tables/overall_results}
}
\end{center}
   \caption{Although humans can answer 97\% of the benchmark questions correctly, existing VideoQA models struggle to achieve a higher than random accuracy.}
\label{table:global}
\end{figure}



\subsection{Spatio-temporal reasoning capabilities}
HCRN performs better than random at questions that ask what object is used by a given relationship and questions that count the number of actions. However, it performs about random on questions asking if a given concept occurred and questions asking to sequence actions, and worse than random on questions that involve the concepts of first, last, and length of actions. 



\textbf{}
\subsection{Question Subject}

Beyond categories on reasoning skills, we also separate the questions by the semantic categories of object, object-relationship, relationships, and actions. Most questions involve more than one of these categories, but we classify questions into these categories by the type of subject queried at the core of the question. 

HCRN performs better than random on open answer questions about objects, but much worse than random on open answer questions about relationships, suggesting that relationships are a more difficult concept to grasp. Binary questions perform about random across all semantic categories, with the exception of actions which perform below random.


\subsection{Question Structure}

We have five types of question structures, that affect the facility of guessing answers. 

\textbf{Query: } Query questions ask for open ended answers on what occurred or on how many times an action occurred. They have the lowest accuracy because they are all open answer. Query questions improve on the random baseline, but the accuracy still remains low.

\textbf{Compare: } Compare questions compare action attributes such as length, number of times an action occurred, or the sequence of actions. HCRN performs poorly on this question structure, achieving an accuracy below random.

\textbf{Choose: } Choose questions choose between two potential options. Although HCRN performs better than random on the choose category, it still performs poorly given that the question offers a choice between two options.

\textbf{Logic: } Logic questions include a logical operator like and or xor. These questions have Yes/No answers. HCRN's accuracy stays at close to 50\% on logic questions.

\textbf{Verify: } Verify questions verify whether or not the content of the question occurred. These questions have Yes/No answers. HCRN's accuracy stays at close to 50\% on verify questions.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/novel comp.pdf}
    \caption{We create a training and test split to measure a model's ability to generalize to novel compositions. In the training set, certain ideas like "before" and "standing up" are seen in many other contexts, but never combined with one another. In the test set, we look specifically at questions where these ideas are combined to see if the accuracy remains consistent with the validation set, implying that the model successfully generalized.}
    \label{fig:compo_fig}
\end{figure}



\subsection{Compositional metrics}

Beyond measuring limited spatio-temporal reasoning, no existing VodeoQA benchmark we know of measures a model's ability to generalize to new information. CLEVRER looks at compositional questions, but does not create metrics designed for measuring compositional reasoning \cite{yi2019clevrer}. %Measuring models on compositional questions measures their ability to generalize from simple building blocks of information to more complex combinations.
These metrics ask if models, when trained on simpler or incomplete information, can successfully complete a more complex task. We adapt strategies from the constrained evaluation of the SCAN dataset \cite{lake2018generalization} to measure models' abilities to generalize to novel combinations of ideas, to adding temporal localizations and indirectly referencing ideas, to videos of longer lengths, and to more complex question structures.


\begin{figure*}[t]
\begin{center}
\input{tables/compositionality_results}
\end{center}
    \caption{We test the model on a variety of compositional reasoning skills. We check it's ability to generalize to novel compositions of ideas (Figure~\ref{fig:compo_fig}, from shorter videos to longer videos, and from simpler question structures to more complex questions structures.}
    \label{table:compo}
\end{figure*}


\begin{figure}[t]
\begin{center}
\resizebox{\linewidth}{!}{
\input{tables/novel_comp_results}
}
\end{center}
   \caption{We check the model's ability to generalize to novel compositions combining the above concepts with an action, object, or relationship.}
\label{table:novel}
\end{figure}


\subsubsection{Novel Composition}

An important part of human reasoning is the ability to generalize concepts to novel compositions. One way to test this ability  is to train the model without a certain combination of phrases, then test it to see if it can still correctly reason about that phrase \cite{lake2018generalization}. For example, in the training set the model may never see the phrase "before standing up", but see both "before" and "standing up" in other contexts. Then, in the test set, we can test if it understands the  idea  of  ”before standing up”. We test combinations of ideas with questions asking the concepts of before $<$action$>$, the first thing they $<$relation$>$, the $<$action$>$ they did for longer, the number of times an $<$action$>$ occurred, and several object-relation pairs. We are able to create such a training and testing split to judge these novel  compositions  because  we  have  knowledge  of what temporal localization, logical reasoning, and indirect tags are in each question.

As shown in Figure~\ref{table:compo}, the model's performance drops considerably from the validation set to the test set when it sees novel combinations of ideas. It especially struggles on questions asking about the number of times something occurred, and with open answer questions involving the concept of "first". 


\begin{figure}[t]
\begin{center}
\resizebox{\linewidth}{!}{
\input{tables/indirects}
}
\end{center}
   \caption{For each question with only direct references, we evaluate the model's performance on identical questions that use indirect references or temporal localizations, categorized by if the direct question was correct or not.}
\label{table:novel}
\end{figure}

\subsubsection{Indirect References}

This metric looks at questions that have  no temporal localization and use only direct references to objects, relationships, and actions. We split these questions by if the model gets them correct or incorrect, then ask a series of other questions with the same base but using indirect references and temporal localizations. 

HCRN struggles especially with relationship indirect references, but shows some consistency along other types of indirect references and with temporal localizations. When it can answer the direct question correctly, it answers more than 65\% of the indirect questions correctly as well. 

\subsubsection{Number of Compositional Steps}

In the previous metric, the model has trained on questions of all complexities. In this metric, we look at if the model is able to generalize to more complex compositional structures after training on simple compositional structures \cite{lake2018generalization}. This metric measures compositional complexity through the number of "steps" each question has, which represents the number of reasoning operations needed to answer it. For each template, we find a the number of steps such that at least 50\% of the questions require that number of steps or fewer. Then, in the training set, we train only on questions that are less than or equal to that marker. In the test set, we only test on questions that are greater than that marker. The marker is high enough that some indirect references are seen in the training set, however in the test set the model must make much more complex combinations and reasoning feats than before. For this metric, we balance the distribution of questions such that for every question in which a temporal localization phrase does not change the answer, there is at least one where it does.

The model's accuracy drops from above random, to at random for binary questions, and decreases ts lead above random for open answer questions.


\subsubsection{Length of video}

Some video models are better at shorter or longer videos. Many existing video datasets are on short videos and many models are inflexible with video length \cite{le2020hierarchical}. This metric tests if models trained on short videos can generalize to being tested on long videos. HCRN was made specifically to be more flexible with video length, and its accuracy only drops slightly when tested on the longer videos.


\section{Discussion}

We contribute a 1.7M question VideoQA benchmark that is grounded in real-world visual concepts, incorporates a wide variety of spatio-temporal reasoning skills, has minimal bias, and can measure different compositional reasoning skills. Tests on existing models struggle on our benchmark, motivating work in developing VideoQA models that are better at compositional reasoning. Future work could expand out from the set of objects, relationships, and actions seen in AGQA and expand to questions with other answer types, such as time stamps, bounding boxes, and lists.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}

\end{document}
